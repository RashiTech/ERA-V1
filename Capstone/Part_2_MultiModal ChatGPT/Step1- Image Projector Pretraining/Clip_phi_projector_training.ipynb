{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY2-2EvbSgtn",
        "outputId": "225bd765-48ff-4954-f2ca-b59d187a1fe3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/capstone-2')"
      ],
      "metadata": {
        "id": "5TKTCDOTSmqG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVDB1ZA6SXU5",
        "outputId": "59c1ca19-b5d6-46f6-c233-cf2bc2639887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IFN4x94Sf4m",
        "outputId": "771f1a68-9a5c-4d68-d1eb-ed1d2920d26d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-26 06:07:33.030178: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-26 06:07:33.030233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-26 06:07:33.031891: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-26 06:07:34.155881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrashiagarwal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/capstone-2/wandb/run-20240126_060737-26rj8a77\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstep1_coco_pretrain\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rashiagarwal/tsai_clip_phi2_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rashiagarwal/tsai_clip_phi2_project/runs/26rj8a77\u001b[0m\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.16it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Train size 532577 and validation size 59176\n",
            "Train size 532577 and validation size 59176\n",
            "Training started.\n",
            "Step 0/50000: Avg Running Loss = 4.957803249359131\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  100\n",
            "Saving Checkpoint for step :  200\n",
            "Saving Checkpoint for step :  300\n",
            "Saving Checkpoint for step :  400\n",
            "Saving Checkpoint for step :  500\n",
            "Saving Checkpoint for step :  600\n",
            "Saving Checkpoint for step :  700\n",
            "Saving Checkpoint for step :  800\n",
            "Saving Checkpoint for step :  900\n",
            "Saving Checkpoint for step :  1000\n",
            "0 - Target captions:\n",
            " A meeting with people holding their laptop and mobile phones.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is using a laptop computer on a desk. keyboard. computer. computer. computer.<|endoftext|> \n",
            "1 - Target captions:\n",
            " NEWBORN BABY SMILING SURROUNDED BY COLORFUL STUFFED ANIMALS  \n",
            "1 - predicted_captions:\n",
            " A baby is sleeping in a bed with a blanket and a. on a. a. a<|endoftext|> \n",
            "Step 1000/50000: Avg Running Loss = 4.968463942050934\n",
            "Saving Checkpoint for step :  1100\n",
            "Saving Checkpoint for step :  1200\n",
            "Saving Checkpoint for step :  1300\n",
            "Saving Checkpoint for step :  1400\n",
            "Saving Checkpoint for step :  1500\n",
            "Saving Checkpoint for step :  1600\n",
            "Saving Checkpoint for step :  1700\n",
            "Saving Checkpoint for step :  1800\n",
            "Saving Checkpoint for step :  1900\n",
            "Saving Checkpoint for step :  2000\n",
            "0 - Target captions:\n",
            " A woman sitting in front of a table with a plate of food.  \n",
            "0 - predicted_captions:\n",
            " A man eating a pizza with a fork and a plate. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A woman is on an indoor court playing tennis.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman playing tennis with a man. a. a. a. a. a. a<|endoftext|> \n",
            "Step 2000/50000: Avg Running Loss = 4.814440055847168\n",
            "Saving Checkpoint for step :  2100\n",
            "Saving Checkpoint for step :  2200\n",
            "Saving Checkpoint for step :  2300\n",
            "Saving Checkpoint for step :  2400\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2500\n",
            "Saving Checkpoint for step :  2600\n",
            "Saving Checkpoint for step :  2700\n",
            "Saving Checkpoint for step :  2800\n",
            "Saving Checkpoint for step :  2900\n",
            "Saving Checkpoint for step :  3000\n",
            "0 - Target captions:\n",
            " A group of children eating lunch at an outdoor picnic table.  \n",
            "0 - predicted_captions:\n",
            " A group of people eating at a table with a table and. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A broken cell phone lies in pieces on asphalt.<|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A small bird is sitting on a small bird. on a. a. a. a.<|endoftext|> \n",
            "Step 3000/50000: Avg Running Loss = 4.664844941377639\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3100\n",
            "Saving Checkpoint for step :  3200\n",
            "Saving Checkpoint for step :  3300\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3400\n",
            "Saving Checkpoint for step :  3500\n",
            "Saving Checkpoint for step :  3600\n",
            "Saving Checkpoint for step :  3700\n",
            "Saving Checkpoint for step :  3800\n",
            "Saving Checkpoint for step :  3900\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  4000\n",
            "0 - Target captions:\n",
            " A man in uniform is holding onto his loved one.   \n",
            "0 - predicted_captions:\n",
            " A group of women are standing in front of a a. a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " Some nice decorations left on a window pane. <|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A small table with a lamp and a lamp on it a a a. a a. a<|endoftext|> \n",
            "Step 4000/50000: Avg Running Loss = 4.646532368659973\n",
            "Saving Checkpoint for step :  4100\n",
            "Saving Checkpoint for step :  4200\n",
            "Saving Checkpoint for step :  4300\n",
            "Saving Checkpoint for step :  4400\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  4500\n",
            "Saving Checkpoint for step :  4600\n",
            "Saving Checkpoint for step :  4700\n",
            "Saving Checkpoint for step :  4800\n",
            "Saving Checkpoint for step :  4900\n",
            "Saving Checkpoint for step :  5000\n",
            "0 - Target captions:\n",
            " Marshmallows being cut into pieces over a bowl with scissors.   \n",
            "0 - predicted_captions:\n",
            " A white bowl with a red and a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A close up of a floral arrangement in a glass vase.<|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A vase with a bouquet of flowers in it on a. a. a. a<|endoftext|> \n",
            "Step 5000/50000: Avg Running Loss = 4.563781486034394\n",
            "Saving Checkpoint for step :  5100\n",
            "Saving Checkpoint for step :  5200\n",
            "Saving Checkpoint for step :  5300\n",
            "Saving Checkpoint for step :  5400\n",
            "Saving Checkpoint for step :  5500\n",
            "Saving Checkpoint for step :  5600\n",
            "Saving Checkpoint for step :  5700\n",
            "Saving Checkpoint for step :  5800\n",
            "Saving Checkpoint for step :  5900\n",
            "Saving Checkpoint for step :  6000\n",
            "0 - Target captions:\n",
            " A clock tower with multiple spires in behind trees.<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A tall building with a flag on the top of a... building. the. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A group of people standing on top of a sandy beach.  \n",
            "1 - predicted_captions:\n",
            " A group of people flying kites in a field.........<|endoftext|> \n",
            "Step 6000/50000: Avg Running Loss = 4.550025860309601\n",
            "Saving Checkpoint for step :  6100\n",
            "Saving Checkpoint for step :  6200\n",
            "Saving Checkpoint for step :  6300\n",
            "Saving Checkpoint for step :  6400\n",
            "Saving Checkpoint for step :  6500\n",
            "Saving Checkpoint for step :  6600\n",
            "Saving Checkpoint for step :  6700\n",
            "Saving Checkpoint for step :  6800\n",
            "Saving Checkpoint for step :  6900\n",
            "Saving Checkpoint for step :  7000\n",
            "0 - Target captions:\n",
            " Broken appliances sitting on a city sidewalk at night<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A row of kitchen appliances in a kitchen........ of a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A very tall clock tower with a clock on it's side.  \n",
            "1 - predicted_captions:\n",
            " A tall clock tower in a city building. tower the. building a. the. clock tower<|endoftext|> \n",
            "Step 7000/50000: Avg Running Loss = 4.528435852766037\n",
            "Saving Checkpoint for step :  7100\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  7200\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  7300\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  7400\n",
            "Saving Checkpoint for step :  7500\n",
            "Saving Checkpoint for step :  7600\n",
            "Saving Checkpoint for step :  7700\n",
            "Saving Checkpoint for step :  7800\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  7900\n",
            "Saving Checkpoint for step :  8000\n",
            "0 - Target captions:\n",
            " A female baby sleeping with her teddy bear.<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A baby with a teddy bear and a.... a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A flip cell phone is shown with a green face.  \n",
            "1 - predicted_captions:\n",
            " A man holding a phone and a laptop on a table...\n",
            "\n",
            ". a table<|endoftext|> \n",
            "Step 8000/50000: Avg Running Loss = 4.562488520622254\n",
            "Saving Checkpoint for step :  8100\n",
            "Saving Checkpoint for step :  8200\n",
            "Saving Checkpoint for step :  8300\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  8400\n",
            "Saving Checkpoint for step :  8500\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  8600\n",
            "Saving Checkpoint for step :  8700\n",
            "Saving Checkpoint for step :  8800\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  8900\n",
            "Saving Checkpoint for step :  9000\n",
            "0 - Target captions:\n",
            " A white refrigerator with the door open with a small amount of food in it.  \n",
            "0 - predicted_captions:\n",
            " A kitchen with a refrigerator, microwave, and a. and a. and a. and a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A boy happily playing tennis on a screened-in court<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man playing tennis on a court a tennis. a. a. a. a. a<|endoftext|> \n",
            "Step 9000/50000: Avg Running Loss = 4.524133143424987\n",
            "Saving Checkpoint for step :  9100\n",
            "Saving Checkpoint for step :  9200\n",
            "Saving Checkpoint for step :  9300\n",
            "Saving Checkpoint for step :  9400\n",
            "Saving Checkpoint for step :  9500\n",
            "Saving Checkpoint for step :  9600\n",
            "Saving Checkpoint for step :  9700\n",
            "Saving Checkpoint for step :  9800\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  9900\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  10000\n",
            "0 - Target captions:\n",
            " A table is covered with vases and containers.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A table with a vase of flowers and a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A table topped with cup up Q tips sitting next to a pair of scissors.  \n",
            "1 - predicted_captions:\n",
            " A small box of scissors and a a a a a a a a. a. a.<|endoftext|> \n",
            "Step 10000/50000: Avg Running Loss = 4.47734967470169\n",
            "Saving Checkpoint for step :  10100\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  10200\n",
            "Saving Checkpoint for step :  10300\n",
            "Saving Checkpoint for step :  10400\n",
            "Saving Checkpoint for step :  10500\n",
            "Saving Checkpoint for step :  10600\n",
            "Saving Checkpoint for step :  10700\n",
            "Saving Checkpoint for step :  10800\n",
            "Saving Checkpoint for step :  10900\n",
            "Saving Checkpoint for step :  11000\n",
            "0 - Target captions:\n",
            " A girl dressed all in orange hitting a tennis ball.  \n",
            "0 - predicted_captions:\n",
            " A woman playing tennis on a court. a tennis racket. a. a. a tennis ball<|endoftext|> \n",
            "1 - Target captions:\n",
            " Three ladies that are all holding teddy bears<|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A group of women standing in front of a a. a teddy bear....<|endoftext|> \n",
            "Step 11000/50000: Avg Running Loss = 4.419537369012833\n",
            "Saving Checkpoint for step :  11100\n",
            "Exception ignored in: <function _xla_gc_callback at 0x7c6940a7a170>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/queue.py\", line 179, in get\n",
            "    raise Empty\n",
            "_queue.Empty\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/capstone-2/main.py\", line 49, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/capstone-2/main.py\", line 41, in main\n",
            "    train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)\n",
            "  File \"/content/drive/MyDrive/capstone-2/network.py\", line 175, in train_model\n",
            "    for batch_idx, (images, target_captions) in enumerate(train_loader):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1284, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1145, in _try_get_data\n",
            "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
            "RuntimeError: DataLoader worker (pid(s) 9634, 9646, 9658, 9670, 9682, 9694, 9706, 9718, 9730, 9742) exited unexpectedly\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▇▄▆▄█▃▅▅▅▃▆▆▃▄█▃▄▄█▅▁▂▅▃▄▄▃▂▃▃▇▃▂▅▃▃▃▂▂▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       step 11182\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 3.26633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstep1_coco_pretrain\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rashiagarwal/tsai_clip_phi2_project/runs/26rj8a77\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240126_060737-26rj8a77/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8BnMC_xS7hk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}