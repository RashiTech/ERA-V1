# Training NanoGPT from scratch

#### Partner from ERAv1- Madhur Prakash Garg

## Hugging face Link

https://huggingface.co/spaces/RashiAgarwal/NanoGPT_charToken

## 1.65 Million Parameter transformer model(Decoder only) trained on 1MB size dataset (Shakespeare work) using character level tokenization

### vocab size: 65

### Train dataset has 1,003,854 tokens

### Validation dataset has 111,540 tokens

## Model trained for 50000 iters resulting in final train loss 0.0685, val loss 3.8487. Overfitting observed

### Training logs

![image](https://github.com/RashiTech/ERA-V1/assets/90626052/4aa6a460-81cd-4683-babd-dffb607149ea)

![Untitled-1](https://github.com/RashiTech/ERA-V1/assets/90626052/5a89ab65-ba60-4f2c-9311-33672ec3d9c4)
