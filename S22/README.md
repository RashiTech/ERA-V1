# The School of AI - GPT model (pythia-160m) with 0.16 Billion parameters trained from Scratch with RedPajama-1T-sample on V100 GPU

### ERAv1 partner Madhur Prakash Garg

## Hugging face app hosted : https://huggingface.co/spaces/RashiAgarwal/TSAIGPTRedPajama

## Training logs
### Loss of 3.5 achieved in   iterations



## Predictions



