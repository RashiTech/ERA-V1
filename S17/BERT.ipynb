{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hl3lv_h74HW","executionInfo":{"status":"ok","timestamp":1694774625657,"user_tz":-330,"elapsed":20034,"user":{"displayName":"Rashi Agarwal","userId":"02602964137408647779"}},"outputId":"438cfdc9-5e82-4b9a-e1ea-b6bdf1935e58"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/S17/Transformer\")\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYIRMhnx8BTG","executionInfo":{"status":"ok","timestamp":1694774625657,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rashi Agarwal","userId":"02602964137408647779"}},"outputId":"62d5472a-8dad-4712-f3b0-27ce45ddc74b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT.ipynb     model.py\t\t  super_repo\t  values.tsv\n","data\t       names.tsv\t  training.txt\t  ViT.ipynb\n","download.jpeg  pizza_steak_sushi  transformer.py  VIT_utils.py\n","GPT.ipynb      __pycache__\t  utils.py\t  vocab.txt\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hTCXP2tmGz1F","outputId":"3de27620-2b11-4001-cc5d-882ab5290ee6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694774669627,"user_tz":-330,"elapsed":43972,"user":{"displayName":"Rashi Agarwal","userId":"02602964137408647779"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["initializing..\n","loading text...\n","tokenizing sentences...\n","creating/loading vocab...\n","creating dataset...\n","initializing model...\n","initializing optimizer and loss...\n","training...\n","it: 0  | loss 10.33  | Δw: 1.178\n","it: 10  | loss 9.6  | Δw: 0.602\n","it: 20  | loss 9.36  | Δw: 0.372\n","it: 30  | loss 9.17  | Δw: 0.286\n","it: 40  | loss 9.07  | Δw: 0.24\n","it: 50  | loss 8.89  | Δw: 0.215\n","it: 60  | loss 8.72  | Δw: 0.192\n","it: 70  | loss 8.53  | Δw: 0.192\n","it: 80  | loss 8.4  | Δw: 0.178\n","it: 90  | loss 8.27  | Δw: 0.164\n","saving embeddings...\n","end\n"]}],"source":["# =============================================================================\n","# Libs\n","# =============================================================================\n","from torch.utils.data import Dataset\n","import torch.nn.functional as F\n","from collections import Counter\n","from os.path import exists\n","import torch.optim as optim\n","import torch.nn as nn\n","import numpy as np\n","import random\n","import torch\n","import math\n","import re\n","from transformer import *\n","\n","\n","# transformer classes --> transformer.py\n","\n","# =============================================================================\n","# Dataset\n","# =============================================================================\n","class SentencesDataset(Dataset):\n","    #Init dataset\n","    def __init__(self, sentences, vocab, seq_len):\n","        dataset = self\n","\n","        dataset.sentences = sentences\n","        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n","        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)}\n","        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n","        dataset.seq_len = seq_len\n","\n","        #special tags\n","        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n","        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n","        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n","\n","\n","    #fetch data\n","    def __getitem__(self, index, p_random_mask=0.15):\n","        dataset = self\n","\n","        #while we don't have enough word to fill the sentence for a batch\n","        s = []\n","        while len(s) < dataset.seq_len:\n","            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n","            index += 1\n","\n","        #ensure that the sequence is of length seq_len\n","        s = s[:dataset.seq_len]\n","        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n","\n","        #apply random mask\n","        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n","\n","        return {'input': torch.Tensor([w[0] for w in s]).long(),\n","                'target': torch.Tensor([w[1] for w in s]).long()}\n","\n","    #return length\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","    #get words id\n","    def get_sentence_idx(self, index):\n","        dataset = self\n","        s = dataset.sentences[index]\n","        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n","        return s\n","\n","# =============================================================================\n","# Methods / Class\n","# =============================================================================\n","def get_batch(loader, loader_iter):\n","    try:\n","        batch = next(loader_iter)\n","    except StopIteration:\n","        loader_iter = iter(loader)\n","        batch = next(loader_iter)\n","    return batch, loader_iter\n","\n","# =============================================================================\n","# #Init\n","# =============================================================================\n","print('initializing..')\n","batch_size = 1024\n","seq_len = 20\n","embed_size = 128\n","inner_ff_size = embed_size * 4\n","n_heads = 8\n","n_code = 8\n","n_vocab = 40000\n","dropout = 0.1\n","# n_workers = 12\n","\n","#optimizer\n","optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n","\n","# =============================================================================\n","# Input\n","# =============================================================================\n","#1) load text\n","print('loading text...')\n","pth = 'training.txt'\n","sentences = open(pth).read().lower().split('\\n')\n","\n","#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n","print('tokenizing sentences...')\n","special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n","sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n","sentences = [[w for w in s if len(w)] for s in sentences]\n","\n","#3) create vocab if not already created\n","print('creating/loading vocab...')\n","pth = 'vocab.txt'\n","if not exists(pth):\n","    words = [w for s in sentences for w in s]\n","    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n","    vocab = [w[0] for w in vocab]\n","    open(pth, 'w+').write('\\n'.join(vocab))\n","else:\n","    vocab = open(pth).read().split('\\n')\n","\n","#4) create dataset\n","print('creating dataset...')\n","dataset = SentencesDataset(sentences, vocab, seq_len)\n","# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n","kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n","data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n","\n","\n","# =============================================================================\n","# Model\n","# =============================================================================\n","#init model\n","print('initializing model...')\n","model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n","model = model.cuda()\n","\n","# =============================================================================\n","# Optimizer\n","# =============================================================================\n","print('initializing optimizer and loss...')\n","optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n","loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n","\n","# =============================================================================\n","# Train\n","# =============================================================================\n","print('training...')\n","print_each = 10\n","model.train()\n","batch_iter = iter(data_loader)\n","n_iteration = 100\n","for it in range(n_iteration):\n","\n","    #get batch\n","    batch, batch_iter = get_batch(data_loader, batch_iter)\n","\n","    #infer\n","    masked_input = batch['input']\n","    masked_target = batch['target']\n","\n","    masked_input = masked_input.cuda(non_blocking=True)\n","    masked_target = masked_target.cuda(non_blocking=True)\n","    output = model(masked_input)\n","\n","    #compute the cross entropy loss\n","    output_v = output.view(-1,output.shape[-1])\n","    target_v = masked_target.view(-1,1).squeeze()\n","    loss = loss_model(output_v, target_v)\n","\n","    #compute gradients\n","    loss.backward()\n","\n","    #apply gradients\n","    optimizer.step()\n","\n","    #print step\n","    if it % print_each == 0:\n","        print('it:', it,\n","              ' | loss', np.round(loss.item(),2),\n","              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n","\n","    #reset gradients\n","    optimizer.zero_grad()\n","\n","\n","# =============================================================================\n","# Results analysis\n","# =============================================================================\n","print('saving embeddings...')\n","N = 3000\n","np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n","s = [dataset.rvocab[i] for i in range(N)]\n","open('names.tsv', 'w+').write('\\n'.join(s) )\n","\n","\n","print('end')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"DXjeBG9UGz1o","executionInfo":{"status":"ok","timestamp":1694774669628,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rashi Agarwal","userId":"02602964137408647779"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}