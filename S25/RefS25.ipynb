{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "outputId": "9fcce421-5e7d-49c4-e451-cf87d639b797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "outputId": "103bebbe-16c8-44df-af22-49772fe1f427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "outputId": "0069b80e-e1f5-432b-88d9-6260e05757f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj"
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg"
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "outputId": "ebd75603-cf61-4732-aa86-da701def1d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "outputId": "01b4c620-92d9-4cc7-f11d-ac8cc11dfdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "\n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "\n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "\n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "\n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "\n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 512.6232988347093\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 493.70176430884203\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 492.40008391554187\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 476.3080060617612\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 494.0312655821267\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 154.195280\n",
            "---------------------------------------\n",
            "Total Timesteps: 5133 Episode Num: 6 Reward: 62.03055212295001\n",
            "Total Timesteps: 6133 Episode Num: 7 Reward: 514.9270628334779\n",
            "Total Timesteps: 6485 Episode Num: 8 Reward: 167.59288140345458\n",
            "Total Timesteps: 7485 Episode Num: 9 Reward: 236.29198633567387\n",
            "Total Timesteps: 8485 Episode Num: 10 Reward: 509.53382938773007\n",
            "Total Timesteps: 8602 Episode Num: 11 Reward: 50.5568507268108\n",
            "Total Timesteps: 9602 Episode Num: 12 Reward: 520.9793005272942\n",
            "Total Timesteps: 10602 Episode Num: 13 Reward: 497.8470933409838\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 128.452366\n",
            "---------------------------------------\n",
            "Total Timesteps: 11602 Episode Num: 14 Reward: 75.12851600337565\n",
            "Total Timesteps: 12602 Episode Num: 15 Reward: 185.85454988972566\n",
            "Total Timesteps: 13602 Episode Num: 16 Reward: 255.98572779187978\n",
            "Total Timesteps: 14602 Episode Num: 17 Reward: 130.0382692472513\n",
            "Total Timesteps: 15602 Episode Num: 18 Reward: 115.21073238051312\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 229.912339\n",
            "---------------------------------------\n",
            "Total Timesteps: 16602 Episode Num: 19 Reward: 286.9407247295469\n",
            "Total Timesteps: 17602 Episode Num: 20 Reward: 227.17287004160997\n",
            "Total Timesteps: 18602 Episode Num: 21 Reward: 80.32986466490651\n",
            "Total Timesteps: 19602 Episode Num: 22 Reward: 283.3712042492783\n",
            "Total Timesteps: 20602 Episode Num: 23 Reward: 127.41396182269945\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 218.761478\n",
            "---------------------------------------\n",
            "Total Timesteps: 21602 Episode Num: 24 Reward: 278.36781644990464\n",
            "Total Timesteps: 22602 Episode Num: 25 Reward: 309.2025085988498\n",
            "Total Timesteps: 23602 Episode Num: 26 Reward: 306.87063698855405\n",
            "Total Timesteps: 24602 Episode Num: 27 Reward: 412.03125738607673\n",
            "Total Timesteps: 25602 Episode Num: 28 Reward: 198.6635895184017\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 350.084413\n",
            "---------------------------------------\n",
            "Total Timesteps: 26602 Episode Num: 29 Reward: 322.19013162263946\n",
            "Total Timesteps: 27602 Episode Num: 30 Reward: 204.21178137830844\n",
            "Total Timesteps: 28602 Episode Num: 31 Reward: 103.61375401208718\n",
            "Total Timesteps: 29602 Episode Num: 32 Reward: 292.5332001401748\n",
            "Total Timesteps: 29631 Episode Num: 33 Reward: 6.9196293063098695\n",
            "Total Timesteps: 29660 Episode Num: 34 Reward: 8.769085282658294\n",
            "Total Timesteps: 29737 Episode Num: 35 Reward: 28.08327579198238\n",
            "Total Timesteps: 29774 Episode Num: 36 Reward: 7.967136280319689\n",
            "Total Timesteps: 30390 Episode Num: 37 Reward: 312.0899631284384\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 202.433665\n",
            "---------------------------------------\n",
            "Total Timesteps: 31390 Episode Num: 38 Reward: 164.12255778235527\n",
            "Total Timesteps: 31447 Episode Num: 39 Reward: 5.823704818050642\n",
            "Total Timesteps: 31548 Episode Num: 40 Reward: 14.783543810558175\n",
            "Total Timesteps: 31695 Episode Num: 41 Reward: 50.982756144854214\n",
            "Total Timesteps: 32695 Episode Num: 42 Reward: 459.96194083486074\n",
            "Total Timesteps: 33695 Episode Num: 43 Reward: 437.38196663630356\n",
            "Total Timesteps: 34398 Episode Num: 44 Reward: 300.1568646720385\n",
            "Total Timesteps: 35398 Episode Num: 45 Reward: 210.21459800639192\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 326.801282\n",
            "---------------------------------------\n",
            "Total Timesteps: 36398 Episode Num: 46 Reward: 316.66459383449904\n",
            "Total Timesteps: 37398 Episode Num: 47 Reward: 339.16414336388686\n",
            "Total Timesteps: 38398 Episode Num: 48 Reward: 106.89385488193517\n",
            "Total Timesteps: 39398 Episode Num: 49 Reward: 379.6430817247807\n",
            "Total Timesteps: 40398 Episode Num: 50 Reward: 334.8962617321453\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 117.635710\n",
            "---------------------------------------\n",
            "Total Timesteps: 41272 Episode Num: 51 Reward: 111.15273857740404\n",
            "Total Timesteps: 41302 Episode Num: 52 Reward: 12.68026840063823\n",
            "Total Timesteps: 41634 Episode Num: 53 Reward: 83.37937865997512\n",
            "Total Timesteps: 41667 Episode Num: 54 Reward: 15.29215813636454\n",
            "Total Timesteps: 41708 Episode Num: 55 Reward: 22.910606202918174\n",
            "Total Timesteps: 42708 Episode Num: 56 Reward: 300.95375435808313\n",
            "Total Timesteps: 43708 Episode Num: 57 Reward: 307.7032024847098\n",
            "Total Timesteps: 44064 Episode Num: 58 Reward: 39.01389233571982\n",
            "Total Timesteps: 44140 Episode Num: 59 Reward: 11.810420664766298\n",
            "Total Timesteps: 45140 Episode Num: 60 Reward: 195.05080454058643\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 232.565050\n",
            "---------------------------------------\n",
            "Total Timesteps: 46140 Episode Num: 61 Reward: 235.29411016742816\n",
            "Total Timesteps: 47140 Episode Num: 62 Reward: 315.8326540369658\n",
            "Total Timesteps: 48140 Episode Num: 63 Reward: 425.615676205701\n",
            "Total Timesteps: 49140 Episode Num: 64 Reward: 628.341676038075\n",
            "Total Timesteps: 50140 Episode Num: 65 Reward: 417.72993234590905\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 483.382864\n",
            "---------------------------------------\n",
            "Total Timesteps: 51140 Episode Num: 66 Reward: 270.6667814230197\n",
            "Total Timesteps: 52140 Episode Num: 67 Reward: 358.8033832631072\n",
            "Total Timesteps: 53140 Episode Num: 68 Reward: 516.792398534641\n",
            "Total Timesteps: 54140 Episode Num: 69 Reward: 403.2419608632971\n",
            "Total Timesteps: 55140 Episode Num: 70 Reward: 440.99007770491835\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 394.578688\n",
            "---------------------------------------\n",
            "Total Timesteps: 55242 Episode Num: 71 Reward: 39.70154329117188\n",
            "Total Timesteps: 56242 Episode Num: 72 Reward: 374.1900584893031\n",
            "Total Timesteps: 57242 Episode Num: 73 Reward: 307.6071611539539\n",
            "Total Timesteps: 58242 Episode Num: 74 Reward: 311.1345148830395\n",
            "Total Timesteps: 59242 Episode Num: 75 Reward: 127.5429629120776\n",
            "Total Timesteps: 60242 Episode Num: 76 Reward: 367.0588217185416\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 108.518749\n",
            "---------------------------------------\n",
            "Total Timesteps: 60419 Episode Num: 77 Reward: 1.2919983145482905\n",
            "Total Timesteps: 61419 Episode Num: 78 Reward: 414.1641433832518\n",
            "Total Timesteps: 62419 Episode Num: 79 Reward: 369.97711832414734\n",
            "Total Timesteps: 63419 Episode Num: 80 Reward: 313.3218709906793\n",
            "Total Timesteps: 64419 Episode Num: 81 Reward: 305.6061976643445\n",
            "Total Timesteps: 65419 Episode Num: 82 Reward: 351.9500421236098\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 462.640597\n",
            "---------------------------------------\n",
            "Total Timesteps: 66419 Episode Num: 83 Reward: 483.3780508358247\n",
            "Total Timesteps: 67419 Episode Num: 84 Reward: 307.4506990402266\n",
            "Total Timesteps: 68419 Episode Num: 85 Reward: 682.4619278143392\n",
            "Total Timesteps: 69419 Episode Num: 86 Reward: 374.5233031917104\n",
            "Total Timesteps: 70419 Episode Num: 87 Reward: 492.08397064197857\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 374.841564\n",
            "---------------------------------------\n",
            "Total Timesteps: 71419 Episode Num: 88 Reward: 442.93137085887406\n",
            "Total Timesteps: 72419 Episode Num: 89 Reward: 586.8272792098666\n",
            "Total Timesteps: 73419 Episode Num: 90 Reward: 358.33666422091284\n",
            "Total Timesteps: 74419 Episode Num: 91 Reward: 621.0003030741502\n",
            "Total Timesteps: 75419 Episode Num: 92 Reward: 674.2112092758323\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 555.177452\n",
            "---------------------------------------\n",
            "Total Timesteps: 76419 Episode Num: 93 Reward: 627.5050249241532\n",
            "Total Timesteps: 77419 Episode Num: 94 Reward: 838.4823478856684\n",
            "Total Timesteps: 78419 Episode Num: 95 Reward: 541.1595708152734\n",
            "Total Timesteps: 79419 Episode Num: 96 Reward: 553.6311493618038\n",
            "Total Timesteps: 80419 Episode Num: 97 Reward: 641.6735821734253\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 504.145333\n",
            "---------------------------------------\n",
            "Total Timesteps: 81419 Episode Num: 98 Reward: 522.8365489351993\n",
            "Total Timesteps: 82419 Episode Num: 99 Reward: 477.0298818993572\n",
            "Total Timesteps: 83419 Episode Num: 100 Reward: 791.6863211157099\n",
            "Total Timesteps: 84419 Episode Num: 101 Reward: 573.3475449740887\n",
            "Total Timesteps: 85419 Episode Num: 102 Reward: 648.3139759060236\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 582.012942\n",
            "---------------------------------------\n",
            "Total Timesteps: 86419 Episode Num: 103 Reward: 467.233995134909\n",
            "Total Timesteps: 87419 Episode Num: 104 Reward: 532.355075793272\n",
            "Total Timesteps: 88419 Episode Num: 105 Reward: 448.2984699517601\n",
            "Total Timesteps: 89419 Episode Num: 106 Reward: 733.0891408655976\n",
            "Total Timesteps: 90419 Episode Num: 107 Reward: 520.7315198264828\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 572.394565\n",
            "---------------------------------------\n",
            "Total Timesteps: 91419 Episode Num: 108 Reward: 247.1330145679398\n",
            "Total Timesteps: 92419 Episode Num: 109 Reward: 539.4933900200043\n",
            "Total Timesteps: 93419 Episode Num: 110 Reward: 482.3074099367543\n",
            "Total Timesteps: 94419 Episode Num: 111 Reward: 608.9362342547292\n",
            "Total Timesteps: 95419 Episode Num: 112 Reward: 533.214044848681\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 432.162784\n",
            "---------------------------------------\n",
            "Total Timesteps: 96419 Episode Num: 113 Reward: 626.7356321953255\n",
            "Total Timesteps: 97419 Episode Num: 114 Reward: 165.0771181649909\n",
            "Total Timesteps: 98419 Episode Num: 115 Reward: 246.41934316667087\n",
            "Total Timesteps: 99419 Episode Num: 116 Reward: 687.2147335807146\n",
            "Total Timesteps: 99552 Episode Num: 117 Reward: 106.12056982992601\n",
            "Total Timesteps: 100552 Episode Num: 118 Reward: 584.3008095812155\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 279.468899\n",
            "---------------------------------------\n",
            "Total Timesteps: 101552 Episode Num: 119 Reward: 251.2864884588421\n",
            "Total Timesteps: 102552 Episode Num: 120 Reward: 474.39117038965736\n",
            "Total Timesteps: 103552 Episode Num: 121 Reward: 359.31576138266746\n",
            "Total Timesteps: 104552 Episode Num: 122 Reward: 314.56947871004485\n",
            "Total Timesteps: 105552 Episode Num: 123 Reward: 591.1184137822272\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 307.921247\n",
            "---------------------------------------\n",
            "Total Timesteps: 106552 Episode Num: 124 Reward: 557.791907911734\n",
            "Total Timesteps: 107552 Episode Num: 125 Reward: 563.0735414812734\n",
            "Total Timesteps: 108552 Episode Num: 126 Reward: 590.9480336821701\n",
            "Total Timesteps: 109552 Episode Num: 127 Reward: 486.69656816088326\n",
            "Total Timesteps: 110552 Episode Num: 128 Reward: 468.3500699716701\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 429.304043\n",
            "---------------------------------------\n",
            "Total Timesteps: 111552 Episode Num: 129 Reward: 411.6801388847237\n",
            "Total Timesteps: 112552 Episode Num: 130 Reward: 411.70937509706556\n",
            "Total Timesteps: 113552 Episode Num: 131 Reward: 372.00569251127206\n",
            "Total Timesteps: 114552 Episode Num: 132 Reward: 666.1430800691087\n",
            "Total Timesteps: 115552 Episode Num: 133 Reward: 460.9904416173104\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 523.570190\n",
            "---------------------------------------\n",
            "Total Timesteps: 116552 Episode Num: 134 Reward: 352.1573019072249\n",
            "Total Timesteps: 117552 Episode Num: 135 Reward: 488.9605787803935\n",
            "Total Timesteps: 118552 Episode Num: 136 Reward: 296.84492677040714\n",
            "Total Timesteps: 119552 Episode Num: 137 Reward: 434.0537955059363\n",
            "Total Timesteps: 120552 Episode Num: 138 Reward: 379.08378272070706\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 657.452964\n",
            "---------------------------------------\n",
            "Total Timesteps: 121552 Episode Num: 139 Reward: 371.07870015375863\n",
            "Total Timesteps: 122552 Episode Num: 140 Reward: 664.9377383113746\n",
            "Total Timesteps: 123552 Episode Num: 141 Reward: 308.3757851862608\n",
            "Total Timesteps: 124552 Episode Num: 142 Reward: 600.4104389421784\n",
            "Total Timesteps: 125552 Episode Num: 143 Reward: 439.525359030352\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 512.187949\n",
            "---------------------------------------\n",
            "Total Timesteps: 126552 Episode Num: 144 Reward: 342.8940278454713\n",
            "Total Timesteps: 127552 Episode Num: 145 Reward: 580.1339780093208\n",
            "Total Timesteps: 128552 Episode Num: 146 Reward: 378.0806987666161\n",
            "Total Timesteps: 129552 Episode Num: 147 Reward: 594.8097622539781\n",
            "Total Timesteps: 130552 Episode Num: 148 Reward: 311.5215678900163\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 529.079285\n",
            "---------------------------------------\n",
            "Total Timesteps: 131552 Episode Num: 149 Reward: 675.7493771261866\n",
            "Total Timesteps: 132552 Episode Num: 150 Reward: 524.108399768893\n",
            "Total Timesteps: 133552 Episode Num: 151 Reward: 592.0693750792075\n",
            "Total Timesteps: 134552 Episode Num: 152 Reward: 292.74064823213683\n",
            "Total Timesteps: 135552 Episode Num: 153 Reward: 330.2225634953474\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 532.268559\n",
            "---------------------------------------\n",
            "Total Timesteps: 136552 Episode Num: 154 Reward: 436.40275488198887\n",
            "Total Timesteps: 137552 Episode Num: 155 Reward: 539.7247665876872\n",
            "Total Timesteps: 138552 Episode Num: 156 Reward: 469.81523890069843\n",
            "Total Timesteps: 139552 Episode Num: 157 Reward: 601.9912032541115\n",
            "Total Timesteps: 140552 Episode Num: 158 Reward: 425.4515105314638\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 314.491185\n",
            "---------------------------------------\n",
            "Total Timesteps: 141468 Episode Num: 159 Reward: 425.93157392282257\n",
            "Total Timesteps: 142468 Episode Num: 160 Reward: 427.26013116609346\n",
            "Total Timesteps: 142833 Episode Num: 161 Reward: 183.91652865733244\n",
            "Total Timesteps: 142947 Episode Num: 162 Reward: 50.069775269034324\n",
            "Total Timesteps: 143947 Episode Num: 163 Reward: 646.364585534303\n",
            "Total Timesteps: 144947 Episode Num: 164 Reward: 477.41824037888335\n",
            "Total Timesteps: 145947 Episode Num: 165 Reward: 552.3042803396013\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 531.458138\n",
            "---------------------------------------\n",
            "Total Timesteps: 146947 Episode Num: 166 Reward: 585.5431257819682\n",
            "Total Timesteps: 147947 Episode Num: 167 Reward: 324.94786041445747\n",
            "Total Timesteps: 148947 Episode Num: 168 Reward: 546.6546656982192\n",
            "Total Timesteps: 149947 Episode Num: 169 Reward: 592.4284350006956\n",
            "Total Timesteps: 150947 Episode Num: 170 Reward: 412.1744292730023\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 536.302828\n",
            "---------------------------------------\n",
            "Total Timesteps: 151947 Episode Num: 171 Reward: 533.0888396635238\n",
            "Total Timesteps: 152947 Episode Num: 172 Reward: 527.3217282008873\n",
            "Total Timesteps: 153947 Episode Num: 173 Reward: 280.43220642595793\n",
            "Total Timesteps: 154947 Episode Num: 174 Reward: 193.33424720378716\n",
            "Total Timesteps: 155947 Episode Num: 175 Reward: 325.8002629464136\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 338.946457\n",
            "---------------------------------------\n",
            "Total Timesteps: 156947 Episode Num: 176 Reward: 290.7160933823586\n",
            "Total Timesteps: 157947 Episode Num: 177 Reward: 379.10298330586903\n",
            "Total Timesteps: 158947 Episode Num: 178 Reward: 292.6396511093699\n",
            "Total Timesteps: 159947 Episode Num: 179 Reward: 461.62048504640205\n",
            "Total Timesteps: 160947 Episode Num: 180 Reward: 422.45777610848126\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 511.189483\n",
            "---------------------------------------\n",
            "Total Timesteps: 161947 Episode Num: 181 Reward: 439.4691276679888\n",
            "Total Timesteps: 162947 Episode Num: 182 Reward: 648.9593381207025\n",
            "Total Timesteps: 163947 Episode Num: 183 Reward: 620.6338059293364\n",
            "Total Timesteps: 164947 Episode Num: 184 Reward: 453.458217124958\n",
            "Total Timesteps: 165947 Episode Num: 185 Reward: 414.36031279219804\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 440.335195\n",
            "---------------------------------------\n",
            "Total Timesteps: 166947 Episode Num: 186 Reward: 522.2063175282304\n",
            "Total Timesteps: 167947 Episode Num: 187 Reward: 259.9825712171355\n",
            "Total Timesteps: 168947 Episode Num: 188 Reward: 520.8221978165838\n",
            "Total Timesteps: 169382 Episode Num: 189 Reward: 185.83971584414329\n",
            "Total Timesteps: 170382 Episode Num: 190 Reward: 674.3617387582459\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 705.455231\n",
            "---------------------------------------\n",
            "Total Timesteps: 171382 Episode Num: 191 Reward: 564.1567976163952\n",
            "Total Timesteps: 172382 Episode Num: 192 Reward: 740.9578833141447\n",
            "Total Timesteps: 173382 Episode Num: 193 Reward: 340.24270992484423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}